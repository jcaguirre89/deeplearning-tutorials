{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Chapters 1 through 4 cover intoduction to neural networks and keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = np.array([[np.zeros(5), np.zeros(5)]*5, [np.ones(5), np.ones(5)]*5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operations\n",
    "```python\n",
    "keras.layers.Dense(512, activation='relu')\n",
    "```\n",
    "This layer can be interpreted as a function, which takes a 2D tensor as\n",
    "input and returns a 2D tensor as output, with shape (*, 512). \n",
    "\n",
    "It implements the operation `output = activation(dot(input, W) + bias)` where\n",
    "W and bias are 1D tensors (vector). \n",
    "\n",
    "## Broadcasting\n",
    "In the specific case of relu (`max(z, 0)`), it is an element-wise operation, as is \n",
    "the addition between tensors. But tensors of different ranks cannot be added element-wise,\n",
    "so the smaller tensor is broadcast.\n",
    "\n",
    "When possible, and if there’s no ambiguity, the smaller tensor will be broadcasted to\n",
    "match the shape of the larger tensor. Broadcasting consists of two steps:\n",
    "1. Axes (called broadcast axes) are added to the smaller tensor to match the ndim of\n",
    "the larger tensor.\n",
    "2. The smaller tensor is repeated alongside these new axes to match the shape of the larger\n",
    "tensor\n",
    "\n",
    "So if the smaller tensor is a vector of weights, these are repeated for each sample in the\n",
    "input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5) (5,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  5.,  5.,  5.,  5.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Broadcasting\n",
    "# x is a 2D tensor of all 1's\n",
    "x = np.array(np.ones((10,5)))\n",
    "# y is a 1D tensor of all 4's\n",
    "y = np.array(np.ones(5)*4)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# However they can still be added together, because numpy broadcasts for you\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.59053941779 1.59053941779\n"
     ]
    }
   ],
   "source": [
    "# Tensor dot (tensor product)\n",
    "# sumproduct of two vectors of same size\n",
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1  # x is a vector\n",
    "    assert len(y.shape) == 1  # y is a vector\n",
    "    assert x.shape[0] == y.shape[0]  # x and y have same dimensions\n",
    "    n_samples = x.shape[0]\n",
    "    z = sum(x[i] * y[i] for i in range(n_samples))\n",
    "    return z\n",
    "\n",
    "x = np.random.random(5)\n",
    "y = np.random.random(5)\n",
    "print(naive_vector_dot(x, y), np.dot(x, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](dot-product.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.04 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100000 loops, best of 3: 3.07 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "naive_vector_dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 34.41 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 658 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "# Re-arrange rows and columns to match a given shape\n",
    "# reshape(6) => [[1, 2, 3], [4, 5, 6]] -> [1, 2, 3, 4, 5, 6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "![alt](sgd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Getting started with NN\n",
    "## Pieces\n",
    "- **Layers**: Data processing module that takes as input one or more tensors\n",
    "and returns as output one or more tensors. Can be `Dense`, `LSTM`, `Conv2D`, etc.\n",
    "- **Models**: Layers are stacked as a network\n",
    "- **Loss functions**: quantity to be minimized during training. Choosing the right objective\n",
    "is critical because the optimizer will take any shortcut it can to minimize it.\n",
    "- **Optimizer**: How the network will be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "## First step: define network\n",
    "Main way to define models is via the Sequential class, which works for only for linear\n",
    "stacks of layers (bar far the most common kind):\n",
    "```python\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape(784,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "## Second step: define loss func and optimization method\n",
    "\n",
    "```python\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "## Last step: fit\n",
    "```python\n",
    "model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
